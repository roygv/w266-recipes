{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveKerasModel(modelName, model):\n",
    "    model_json = model.to_json()\n",
    "    with open(modelName, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "def loadKerasModel(modelName):\n",
    "    json_file = open(modelName, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://RecipeVectors/unique_ingredients.pkl to ./unique_ingredients.pkl\n"
     ]
    }
   ],
   "source": [
    "!sudo aws s3 cp s3://RecipeVectors/unique_ingredients.pkl .\n",
    "!sudo aws s3 cp s3://RecipeVectors/CleanedIngredients.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('unique_ingredients.pkl', 'rb') as f:\n",
    "    unique_ingredients = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('CleanedIngredients.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model 1, Categories Clustering as Label\n",
    "\n",
    "Since a recipe can belong to multiple categories, and it is logical for a recipe to belong in multiple categories(i.e. a medium-rare steak can fit in following categories: beef, dinner, meat etc.) We first cluster the categories and assign each recipe with its respective index. \n",
    "\n",
    "In the subsequent classification, we will use the cluster index as the Y label, and then retrieve the centroid corresponding to the cluster index to output the general categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13592    beef , appet , meat , poultry , appet , appeti...\n",
       "6638     japanese , asian , salmon , fish , everyday , ...\n",
       "3452     scandinavian , norwegian , norwegian , cooki ,...\n",
       "451      squash , side , roasted , veget , vegetable , ...\n",
       "3828     roasted , veget , brussels , sprouts , side , ...\n",
       "Name: allCategories, dtype: object"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import *\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "USELESS_CATEGORIES = set(map(stemmer.stem, ['recipes', 'and','main','dishes', 'cooking']))\n",
    "\n",
    "def removeUselessCategories(arr):\n",
    "    \n",
    "    n_arr = []\n",
    "    for category in arr:\n",
    "        st_cat = stemmer.stem(category)\n",
    "        for word in st_cat.split():\n",
    "            if not word in USELESS_CATEGORIES:\n",
    "                n_arr.append(word)\n",
    "    return ' , '.join(n_arr)\n",
    "df_cluster = df.copy()\n",
    "df_cluster['allCategories'] = df_cluster['categories'].apply(removeUselessCategories)\n",
    "df_cluster['allCategories'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " cooki\n",
      " dessert\n",
      " fruit\n",
      " international\n",
      " drop\n",
      " bar\n",
      " christmas\n",
      " italian\n",
      " chocolate\n",
      " nut\n",
      "\n",
      "Cluster 1:\n",
      " appet\n",
      " spread\n",
      " dips\n",
      " cheese\n",
      " snack\n",
      " appetizers\n",
      " salsa\n",
      " dip\n",
      " mexican\n",
      " bean\n",
      "\n",
      "Cluster 2:\n",
      " pasta\n",
      " shap\n",
      " italian\n",
      " minute\n",
      " chees\n",
      " lasagna\n",
      " everyday\n",
      " macaroni\n",
      " vegetarian\n",
      " 30\n",
      "\n",
      "Cluster 3:\n",
      " soup\n",
      " soups\n",
      " chili\n",
      " stew\n",
      " stews\n",
      " canned\n",
      " chicken\n",
      " pea\n",
      " everyday\n",
      " beef\n",
      "\n",
      "Cluster 4:\n",
      " chicken\n",
      " breast\n",
      " baked\n",
      " roasted\n",
      " everyday\n",
      " low\n",
      " asian\n",
      " style\n",
      " minute\n",
      " pasta\n",
      "\n",
      "Cluster 5:\n",
      " seafood\n",
      " everyday\n",
      " vegetable\n",
      " low\n",
      " potato\n",
      " fish\n",
      " canned\n",
      " salmon\n",
      " squash\n",
      " mexican\n",
      "\n",
      "Cluster 6:\n",
      " vegetarian\n",
      " indian\n",
      " vegan\n",
      " pea\n",
      " bean\n",
      " grain\n",
      " everyday\n",
      " calorie\n",
      " asian\n",
      " tofu\n",
      "\n",
      "Cluster 7:\n",
      " cooki\n",
      " dessert\n",
      " chocolate\n",
      " christmas\n",
      " extract\n",
      " vanilla\n",
      " browni\n",
      " drop\n",
      " bar\n",
      " extracts\n",
      "\n",
      "Cluster 8:\n",
      " breakfast\n",
      " brunch\n",
      " egg\n",
      " potato\n",
      " seafood\n",
      " pancak\n",
      " meat\n",
      " casserol\n",
      " bread\n",
      " everyday\n",
      "\n",
      "Cluster 9:\n",
      " cak\n",
      " dessert\n",
      " cake\n",
      " mix\n",
      " chocolate\n",
      " holiday\n",
      " fruit\n",
      " pumpkin\n",
      " cupcak\n",
      " everyday\n",
      "\n",
      "Cluster 10:\n",
      " dress\n",
      " stuffing\n",
      " thanksgiving\n",
      " sausage\n",
      " fall\n",
      " bread\n",
      " everyday\n",
      " oyster\n",
      " apple\n",
      " cornbread\n",
      "\n",
      "Cluster 11:\n",
      " drink\n",
      " smoothi\n",
      " cocktail\n",
      " breakfast\n",
      " ingredient\n",
      " punch\n",
      " banana\n",
      " fruit\n",
      " everyday\n",
      " strawberry\n",
      "\n",
      "Cluster 12:\n",
      " dessert\n",
      " pi\n",
      " fruit\n",
      " pie\n",
      " candi\n",
      " chocolate\n",
      " everyday\n",
      " apple\n",
      " specialty\n",
      " cream\n",
      "\n",
      "Cluster 13:\n",
      " pork\n",
      " chop\n",
      " calorie\n",
      " rib\n",
      " steaks\n",
      " low\n",
      " 300\n",
      " cooker\n",
      " slow\n",
      " roast\n",
      "\n",
      "Cluster 14:\n",
      " salad\n",
      " pasta\n",
      " chicken\n",
      " vegetable\n",
      " everyday\n",
      " summer\n",
      " potato\n",
      " green\n",
      " lettuce\n",
      " veget\n",
      "\n",
      "Cluster 15:\n",
      " appet\n",
      " snack\n",
      " appetizers\n",
      " poultry\n",
      " meat\n",
      " cheese\n",
      " everyday\n",
      " big\n",
      " game\n",
      " easy\n",
      "\n",
      "Cluster 16:\n",
      " bread\n",
      " muffin\n",
      " quick\n",
      " breakfast\n",
      " yeast\n",
      " holiday\n",
      " brunch\n",
      " pumpkin\n",
      " everyday\n",
      " fruit\n",
      "\n",
      "Cluster 17:\n",
      " casserol\n",
      " beef\n",
      " noodle\n",
      " ground\n",
      " pasta\n",
      " chicken\n",
      " vegetable\n",
      " mexican\n",
      " everyday\n",
      " canned\n",
      "\n",
      "Cluster 18:\n",
      " beef\n",
      " steak\n",
      " ground\n",
      " carb\n",
      " low\n",
      " mexican\n",
      " everyday\n",
      " meatloaf\n",
      " bbq\n",
      " calorie\n",
      "\n",
      "Cluster 19:\n",
      " turkey\n",
      " thanksgiving\n",
      " leftov\n",
      " ground\n",
      " everyday\n",
      " sandwich\n",
      " burg\n",
      " burger\n",
      " july\n",
      " 4th\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def makePipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words='english')),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', KMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def trainModel(X):\n",
    "    model = makePipeline()\n",
    "    model.fit(X)\n",
    "    return model\n",
    "\n",
    "model = trainModel(df_cluster['allCategories'].values)\n",
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "vectorizer = model.named_steps['vect']\n",
    "km = model.named_steps['clf']\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "true_k = 20\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print \"Cluster %d:\" % i\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print ' %s' % terms[ind]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the cluster index to the model\n",
    "df['category_cluster_index'] = model.predict(df['allCategories'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3960      2\n",
       "14340     9\n",
       "2202      2\n",
       "3933      2\n",
       "2256     11\n",
       "Name: category_cluster_index, dtype: int32"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_cluster_index'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model on the 20 cluster indexes\n",
    "Here we are using the cluster indices as labels and concating the ingredient list together to create the features. We then train a RNN classification model. The first layer of our sequential model use the embedding of size 100, the second layer is a LSTM layer, we also added a dropout layer of 0.4. The final layer is a softmax layer to map it to one of our 20 indices. \n",
    "\n",
    "We run the model for 10 epochs and achieved an accuracy of 0.78. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] num of labels: 20\n",
      "[ 1  1  5  0  4  3 16  6  4 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuqinghe/anaconda/envs/py27/lib/python2.7/site-packages/keras/preprocessing/text.py:139: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5486 unique tokens.\n",
      "('Shape of data tensor:', (89061, 100))\n",
      "('Shape of label tensor:', (89061,))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 1,082,420\n",
      "Trainable params: 1,082,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "71249/71249 [==============================] - 474s - loss: 2.1189 - acc: 0.3357   \n",
      "Epoch 2/10\n",
      "71249/71249 [==============================] - 419s - loss: 1.5078 - acc: 0.5331   \n",
      "Epoch 3/10\n",
      "71249/71249 [==============================] - 417s - loss: 1.2719 - acc: 0.6101   \n",
      "Epoch 4/10\n",
      "71249/71249 [==============================] - 414s - loss: 1.1060 - acc: 0.6605   \n",
      "Epoch 5/10\n",
      "71249/71249 [==============================] - 527s - loss: 1.0018 - acc: 0.6907   \n",
      "Epoch 6/10\n",
      "71249/71249 [==============================] - 486s - loss: 0.9218 - acc: 0.7137   \n",
      "Epoch 7/10\n",
      "71249/71249 [==============================] - 467s - loss: 0.8562 - acc: 0.7363   \n",
      "Epoch 8/10\n",
      "71249/71249 [==============================] - 433s - loss: 0.8028 - acc: 0.7504   \n",
      "Epoch 9/10\n",
      "71249/71249 [==============================] - 427s - loss: 0.7532 - acc: 0.7645   \n",
      "Epoch 10/10\n",
      "71249/71249 [==============================] - 871s - loss: 0.7029 - acc: 0.7799   \n",
      "17812/17812 [==============================] - 115s   \n",
      "[0.81701629787730334, 0.74814731644266996]\n"
     ]
    }
   ],
   "source": [
    "def concatIngredients(arr):\n",
    "    return ','.join(arr).encode('ascii', 'ignore')\n",
    "\n",
    "def processKerasModel(df_):\n",
    "    embedding_length = 100\n",
    "    top_words = 10000\n",
    "    df = df_.copy()\n",
    "    df['features'] = df['ingredients'].apply(concatIngredients)\n",
    "    df['label'] = df['category_cluster_index']\n",
    "    df = df[df['label'].astype(str) != 'nan']\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    features = df['features'].values\n",
    "    labels = df['label'].values\n",
    "    \n",
    "    le.fit(np.unique(labels))\n",
    "    print list(le.classes_), 'num of labels:',len(np.unique(labels))\n",
    "    labels = le.transform(labels) \n",
    "    print labels[:10]\n",
    "#     X_train, Y_train, X_test, Y_test = ut.simpleSplit(features, labels)\n",
    "\n",
    "    tokenizer = Tokenizer(nb_words=top_words)\n",
    "    tokenizer.fit_on_texts(features)\n",
    "    sequences = tokenizer.texts_to_sequences(features)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=embedding_length)\n",
    "\n",
    "#     labels = np_utils.to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    # split the data into a training set and a validation set\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "    X_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    X_test = data[-nb_validation_samples:]\n",
    "    y_test = labels[-nb_validation_samples:]\n",
    "    \n",
    "    embedding_vecor_length = 100\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vecor_length, input_length=embedding_length))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(LSTM(100)) #, return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dense(len(np.unique(labels)), activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "    # Final evaluation of the model\n",
    "    return model, X_test, y_test\n",
    "\n",
    "rnn_cluster_model, X_test, y_test = processKerasModel(df_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy on the train set is 0.78, the accuracy on the test set is 0.75. In total the model took about 2 hours to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model for future use\n",
    "saveKerasModel('model_top_20_clustering_index.json',rnn_cluster_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model if needed\n",
    "# t_model = loadKerasModel('model_top_20_clustering_index.json')\n",
    "# t_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification  Model 2: Top 30 Categories by Index\n",
    "In this model, we have first extract the top 30 categories by doing counting the occurency of each category. Then we set the label of each recipe to one of the top 30 categories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "unique_cat = Counter()\n",
    "USELESS_CATEGORIES = ['recipes']\n",
    "for idx, row in df.iterrows():\n",
    "    for cat in row['categories']:\n",
    "        cat = cat.lower()\n",
    "        if not cat in USELESS_CATEGORIES:\n",
    "            unique_cat[cat]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'everyday cooking',\n",
       " u'main dishes',\n",
       " u'desserts',\n",
       " u'appetizers and snacks',\n",
       " u'pasta by shape',\n",
       " u'chicken breasts',\n",
       " u'asian recipes',\n",
       " u'side dishes',\n",
       " u'pasta main dishes',\n",
       " u'soups, stews and chili',\n",
       " u'fruit desserts',\n",
       " u'italian recipes',\n",
       " u'salad recipes',\n",
       " u'bread recipes',\n",
       " u'breakfast and brunch',\n",
       " u'mexican recipes',\n",
       " u'cake recipes',\n",
       " u'faceless recipes',\n",
       " u'drinks',\n",
       " u'cookies',\n",
       " u'soup',\n",
       " u'trusted brands',\n",
       " u'baked and roasted chicken',\n",
       " u'dips and spreads',\n",
       " u'vegan recipes',\n",
       " u'fish recipes',\n",
       " u'casseroles',\n",
       " u'canned beans and peas',\n",
       " u'vanilla extract',\n",
       " u'cheese appetizers']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_K_CAT = 30\n",
    "k_cat = [cat[0] for cat in unique_cat.most_common(TOP_K_CAT)]\n",
    "k_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map each recipe to a category\n",
    "since each recipe contains multiple categories, we loop through the categories list for each recipe, and map the recipe to ONE category in the TOP_K_CAT list. If a recipe doesn't belong in any category, we map it to 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "everyday cooking             12170\n",
       "side dishes                   5847\n",
       "fruit desserts                4884\n",
       "salad recipes                 4733\n",
       "asian recipes                 4434\n",
       "chicken breasts               4350\n",
       "breakfast and brunch          3999\n",
       "italian recipes               3761\n",
       "appetizers and snacks         3679\n",
       "desserts                      3633\n",
       "trusted brands                3416\n",
       "soup                          3366\n",
       "bread recipes                 3270\n",
       "cookies                       3180\n",
       "dips and spreads              3033\n",
       "pasta by shape                3030\n",
       "drinks                        2903\n",
       "cake recipes                  2490\n",
       "main dishes                   2264\n",
       "mexican recipes               2220\n",
       "soups, stews and chili        2099\n",
       "fish recipes                  1812\n",
       "cheese appetizers             1318\n",
       "casseroles                    1201\n",
       "baked and roasted chicken      675\n",
       "pasta main dishes              497\n",
       "vegan recipes                  440\n",
       "other                          348\n",
       "vanilla extract                  9\n",
       "Name: cat_label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapToSingleCategory(arr):\n",
    "    for item in arr:\n",
    "        if item.lower() in k_cat:\n",
    "            return item.lower()\n",
    "    return 'other'\n",
    "df_single_cat = df.copy()\n",
    "df_single_cat['cat_label'] = df_single_cat['categories'].apply(mapToSingleCategory)\n",
    "df_single_cat['cat_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embedding using the unique_ingredients\n",
    "We have already created a list of unique_ingredients, we have already create the corresponding MxN embedding matrix, where M = number of unique recipes, and N = the embedding size. In our case, we have M = 18996 unique ingredients.\n",
    "\n",
    "This takes a long time so we processed and stored the word embedding matrix in sparse_recipe_ingredient_matrix.npz. We just need to map those to the categories for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reduced_cooccurrence_matrix.pkl', 'rb') as f:\n",
    "    reduced_cooccurrence_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'Unique Ingredients: ',len(unique_ingredients)\n",
    "print 'Embedding matrix shape: ',reduced_cooccurrence_matrix.shape\n",
    "def createIndexMappingAndWordVec():\n",
    "    word_to_idx_mapping = {}\n",
    "    word_vec = {}\n",
    "    for idx, word in enumerate(unique_ingredients):\n",
    "        word_to_idx_mapping[word] = idx\n",
    "        word_vec[word] = reduced_cooccurrence_matrix[idx]\n",
    "    return word_to_idx_mapping, word_vec\n",
    "word_to_idx_mapping, word_vec = createIndexMappingAndWordVec()\n",
    "\n",
    "# test the embedding is working:\n",
    "print 'Index for ingredient(lessfat cream cheese):',  word_to_idx_mapping['lessfat cream cheese']\n",
    "print 'embedding vector for ingredient(lessfat cream cheese): ', word_vec['lessfat cream cheese']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the word_to_idx_mapping and word_vec, we must iterate over each recipe, and for each ingredient list, map the ingredient to its corresponding index in the word_to_idx_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseDataset(ingredientList):\n",
    "    ingredientsIndexList = []\n",
    "    for ingredient in ingredientList:\n",
    "        try:\n",
    "            ingredientsIndexList.append(word_to_idx_mapping[ingredient])\n",
    "        except:\n",
    "            ingredientsIndexList.append(0)\n",
    "    return ingredientsIndexList\n",
    "df_single_cat['cleanedIngredientsIndexedList'] = df_single_cat['cleanedIngredients'].apply(parseDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanedIngredients               [nonfat plain yogurt, peanut butter, banana, s...\n",
      "cleanedIngredientsIndexedList                     [11287, 12081, 873, 16247, 8249]\n",
      "Name: 4972, dtype: object\n",
      "11287\n"
     ]
    }
   ],
   "source": [
    "# check that we have the correct indexed list\n",
    "print df_single_cat.iloc[3739][['cleanedIngredients','cleanedIngredientsIndexedList']]\n",
    "print word_to_idx_mapping['nonfat plain yogurt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_single_cat['features'] = df_single_cat['cleanedIngredientsIndexedList']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode the labels\n",
    "Here we use sklearn's preprocessing to encode the 30 labels into indices; the final softmax layer in our classification model will be a vector of length 30, corresponding to the probability of each label for a given input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'appetizers and snacks' u'asian recipes' u'baked and roasted chicken'\n",
      " u'bread recipes' u'breakfast and brunch' u'cake recipes' u'casseroles'\n",
      " u'cheese appetizers' u'chicken breasts' u'cookies' u'desserts'\n",
      " u'dips and spreads' u'drinks' u'everyday cooking' u'fish recipes'\n",
      " u'fruit desserts' u'italian recipes' u'main dishes' u'mexican recipes'\n",
      " 'other' u'pasta by shape' u'pasta main dishes' u'salad recipes'\n",
      " u'side dishes' u'soup' u'soups, stews and chili' u'trusted brands'\n",
      " u'vanilla extract' u'vegan recipes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_single_cat['cat_label'])\n",
    "print le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>[13113, 14761, 6387, 2502, 10753]</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7910</th>\n",
       "      <td>[12894, 2834, 9741, 14742, 2502, 16106]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14197</th>\n",
       "      <td>[1141, 14745, 18123, 16820, 9498, 15865]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15709</th>\n",
       "      <td>[11101, 17879, 2205, 11101, 5778, 6099, 6091, ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>[3332, 14761, 11532, 2956, 3055, 13113, 18123]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                features  label\n",
       "506                    [13113, 14761, 6387, 2502, 10753]     23\n",
       "7910             [12894, 2834, 9741, 14742, 2502, 16106]      0\n",
       "14197           [1141, 14745, 18123, 16820, 9498, 15865]     13\n",
       "15709  [11101, 17879, 2205, 11101, 5778, 6099, 6091, ...     23\n",
       "2971      [3332, 14761, 11532, 2956, 3055, 13113, 18123]     13"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single_cat['label'] = le.transform(df_single_cat['cat_label'])\n",
    "df_final = df_single_cat[['features','label']]\n",
    "df_final.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'casseroles', u'drinks', u'appetizers and snacks'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the encoding works\n",
    "le.inverse_transform([6,12,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalized Dataset\n",
    "The final dataset is split into Train/Test sets,  we can now pipe this through a CNN model with softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the variables\n",
    "VOCAB_SIZE = len(word_to_idx_mapping)\n",
    "EMBEDDING_SIZE = 50\n",
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (71248, 50)\n",
      "y_train shape: (71248,)\n",
      "X_test shape: (17813, 50)\n",
      "y_test shape: (17813,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "features = np.array([np.array(row) for row in df_final['features'].values])\n",
    "labels = df_final['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_LEN)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_LEN)\n",
    "\n",
    "print 'X_train shape:', X_train.shape\n",
    "print 'y_train shape:', y_train.shape\n",
    "print 'X_test shape:', X_test.shape\n",
    "print 'y_test shape:', y_test.shape\n",
    "# print 'sample X:', np.random.choice(X_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (71248, 50))\n",
      "('Shape of label tensor:', (71248,))\n",
      "Training model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_19 (Embedding)     (None, 50, 50)            949800    \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 46, 128)           32128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 9, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 29)                3741      \n",
      "=================================================================\n",
      "Total params: 1,133,253.0\n",
      "Trainable params: 183,453.0\n",
      "Non-trainable params: 949,800.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "71248/71248 [==============================] - 15s - loss: 13.0967 - acc: 0.1417    \n",
      "Epoch 2/10\n",
      "71248/71248 [==============================] - 16s - loss: 3.2632 - acc: 0.1482    \n",
      "Epoch 3/10\n",
      "71248/71248 [==============================] - 16s - loss: 3.0780 - acc: 0.1493    \n",
      "Epoch 4/10\n",
      "71248/71248 [==============================] - 16s - loss: 3.0739 - acc: 0.1491    \n",
      "Epoch 5/10\n",
      "71248/71248 [==============================] - 15s - loss: 3.0714 - acc: 0.1500    \n",
      "Epoch 6/10\n",
      "71248/71248 [==============================] - 15s - loss: 3.0726 - acc: 0.1500    \n",
      "Epoch 7/10\n",
      "71248/71248 [==============================] - 16s - loss: 3.0694 - acc: 0.1506    \n",
      "Epoch 8/10\n",
      "71248/71248 [==============================] - 15s - loss: 3.0662 - acc: 0.1511    \n",
      "Epoch 9/10\n",
      "71248/71248 [==============================] - 14s - loss: 3.0625 - acc: 0.1515    \n",
      "Epoch 10/10\n",
      "71248/71248 [==============================] - 15s - loss: 3.0655 - acc: 0.1515    \n"
     ]
    }
   ],
   "source": [
    "# global max pooling : https://www.quora.com/What-is-global-average-pooling\n",
    "# CNN for text classification: https://arxiv.org/pdf/1408.5882.pdf\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def trainKerasModel_conv(X,Y):\n",
    "    print('Shape of data tensor:', X.shape)\n",
    "    print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "    print('Training model.')\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "    embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                        EMBEDDING_SIZE,\n",
    "                        weights=[reduced_cooccurrence_matrix],\n",
    "                        input_length=MAX_LEN,\n",
    "                        trainable=False)\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(len(le.classes_), activation='softmax')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['acc'])\n",
    "    print model.summary()\n",
    "    model.fit(X, Y, epochs=10, batch_size=MAX_LEN)\n",
    "    # Final evaluation of the model\n",
    "    return model\n",
    "\n",
    "conv_model = trainKerasModel_conv(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance resulted in a relatively terrible accuracy of 0.15. This may be due to the fact we have already created the embedding weight matrix and disabled changing the matrix during the training. Overall I don't believe we will use a Convolutional Model for our classification problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word to vec using gensim: https://radimrehurek.com/gensim/models/word2vec.html  \n",
    "Here we are creating the word vectors from the list of clean ingredients. We use the Gensim model to create the similarity matrix(cosine similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_ingredients = df['cleanedIngredients'].map(lambda ingList: ' , '.join(ingList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'skinless boneless chicken thighs , teriyaki sauce , tomatobased chili sauce , servings butterflavored cooking spray , deli ham , lowfat string cheese , toothpicks , panko bread crumbs',\n",
       "       'olive oil , salt black pepper , boneless pork chops , riesling wine , under peaches , cinnamon , nutmeg , brown sugar',\n",
       "       'fusilli pasta , extravirgin olive oil , green onions , garlic , kale , cherry tomatoes , zucchini , salt black pepper , rosemary , belgioioso mascarpone cheese , almonds , belgioioso parmesan cheese'], dtype=object)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(combined_ingredients,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Word2vec model...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "print('Training a Word2vec model...')\n",
    "w2v_model = Word2Vec(df['cleanedIngredients'].values, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('onions', 0.8862268924713135),\n",
       " ('yellow onion', 0.8533211946487427),\n",
       " ('sweet onion', 0.780663013458252),\n",
       " ('white onion', 0.7551392316818237),\n",
       " ('yellow onions', 0.699718713760376),\n",
       " ('onion rings', 0.6861737966537476),\n",
       " ('sweet onions', 0.6645956635475159),\n",
       " ('sweet onion vidali', 0.6365644931793213),\n",
       " ('red onion', 0.6306204795837402),\n",
       " ('onions rings', 0.6101366281509399)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['onion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scallions', 0.5430881381034851),\n",
       " ('skinless boneless chicken breast', 0.49212953448295593),\n",
       " ('bean sprouts', 0.4781082272529602),\n",
       " ('chicken breast', 0.4766371250152588),\n",
       " ('boneless skinless chicken breast', 0.47488290071487427),\n",
       " ('green onions', 0.4744403064250946),\n",
       " ('red bell peppers', 0.4724377691745758),\n",
       " ('flank steak', 0.4641454815864563),\n",
       " ('snow peas', 0.45565730333328247),\n",
       " ('skinless boneless chicken breast meat', 0.4532751441001892)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['chicken'], negative=['eggs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model 3\n",
    "Let's do something fun and generate the recipes from scratch :D\n",
    "\n",
    "Inspired By: https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'categories', u'cookingTime', u'description', u'ingredients',\n",
       "       u'instructionSteps', u'name', u'rating', u'ratingCount', u'url',\n",
       "       'cookingTimeMinutes', 'cleanedIngredients'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('CleanedIngredients.pkl')\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def processCategories(arr):\n",
    "    arr = [re.sub(r'[\\xa0\\x99]', ' ', k.encode('ascii', 'ignore')) for k in arr]\n",
    "    return '<category> ' + ','.join(arr) + ' </category>'\n",
    "def processIngredients(arr):\n",
    "    arr = [re.sub(r'[\\xa0\\x99]', ' ', k.encode('ascii', 'ignore')) for k in arr]\n",
    "    return '<ingredients> ' + ','.join(arr) + ' </ingredients>'\n",
    "def processName(text):\n",
    "    return '<name> '+ text+' </name>'\n",
    "def processDesc(text):\n",
    "    if text == text:\n",
    "        text = re.sub(r'[\\xa0\\x99]', ' ', text.encode('ascii', 'ignore'))\n",
    "        return '<desc> '+ str(text) +' </desc>'\n",
    "    return '<no_desc>'\n",
    "def processInstructions(arr):\n",
    "    if not arr == arr:\n",
    "        return '<no_inst>'\n",
    "    retStr = '<inst> '\n",
    "    for i, inst in enumerate(arr):\n",
    "        inst = re.sub(r'[\\xa0\\x99]', ' ', inst.encode('ascii', 'ignore'))\n",
    "        retStr += (str(i) +' : '+inst + ' ')\n",
    "    retStr += ' </inst>'\n",
    "    return retStr\n",
    "def getRating(text):\n",
    "    return '<rating> ' + str(text) + ' </rating>' \n",
    "def addRowsTogether(row):\n",
    "    return ' '.join(row[['ingredients', 'name','instructionSteps']])\n",
    "\n",
    "\n",
    "df_recipe_generator = df.copy()\n",
    "df_recipe_generator['categories'] = df_recipe_generator['categories'].apply(processCategories)\n",
    "df_recipe_generator['name'] = df_recipe_generator['name'].apply(processName)\n",
    "df_recipe_generator['ingredients'] = df_recipe_generator['ingredients'].apply(processIngredients)\n",
    "df_recipe_generator['description'] = df_recipe_generator['description'].apply(processDesc)\n",
    "df_recipe_generator['rating'] = df_recipe_generator['rating'].apply(getRating)\n",
    "df_recipe_generator['instructionSteps'] = df_recipe_generator['instructionSteps'].apply(processInstructions)\n",
    "\n",
    "df_recipe_generator['total_recipe'] = df_recipe_generator.apply(addRowsTogether, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7211     <ingredients> 3 tablespoons olive oil,1 pound ...\n",
       "10220    <ingredients> 1 teaspoon vegetable oil,1 shall...\n",
       "4104     <ingredients> 4 slices whole-grain bread, ligh...\n",
       "23550    <ingredients> 1/3 cup soy sauce,2/3 cup white ...\n",
       "12382    <ingredients> 2 tablespoons butter,5 slices ba...\n",
       "Name: total_recipe, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recipe_generator['total_recipe'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df_recipe_generator['total_recipe'].values\n",
    "chars = list(set(data))\n",
    "VOCAB_SIZE = len(chars)\n",
    "SEQ_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total chars:', 45211)\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb sequences:', 29674)\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(data) - maxlen, step):\n",
    "    sentences.append(data[i: i + maxlen])\n",
    "    next_chars.append(data[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]]\n"
     ]
    }
   ],
   "source": [
    "print y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "()\n",
      "--------------------------------------------------\n",
      "('Iteration', 1)\n",
      "Epoch 1/1\n",
      " 8448/29674 [=======>......................] - ETA: 1750s - loss: 10.9706"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
