{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveKerasModel(modelName, model):\n",
    "    model_json = model.to_json()\n",
    "    with open(modelName, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "def loadKerasModel(modelName):\n",
    "    json_file = open(modelName, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://RecipeVectors/unique_ingredients.pkl to ./unique_ingredients.pkl\n"
     ]
    }
   ],
   "source": [
    "!sudo aws s3 cp s3://RecipeVectors/unique_ingredients.pkl .\n",
    "!sudo aws s3 cp s3://RecipeVectors/CleanedIngredients.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_ingredients.pkl', 'rb') as f:\n",
    "    unique_ingredients = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('CleanedIngredients.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model 1, Categories Clustering as Label\n",
    "\n",
    "Since a recipe can belong to multiple categories, and it is logical for a recipe to belong in multiple categories(i.e. a medium-rare steak can fit in following categories: beef, dinner, meat etc.) We first cluster the categories and assign each recipe with its respective index. \n",
    "\n",
    "In the subsequent classification, we will use the cluster index as the Y label, and then retrieve the centroid corresponding to the cluster index to output the general categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13592    beef , appet , meat , poultry , appet , appeti...\n",
       "6638     japanese , asian , salmon , fish , everyday , ...\n",
       "3452     scandinavian , norwegian , norwegian , cooki ,...\n",
       "451      squash , side , roasted , veget , vegetable , ...\n",
       "3828     roasted , veget , brussels , sprouts , side , ...\n",
       "Name: allCategories, dtype: object"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import *\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "USELESS_CATEGORIES = set(map(stemmer.stem, ['recipes', 'and','main','dishes', 'cooking']))\n",
    "\n",
    "def removeUselessCategories(arr):\n",
    "    \n",
    "    n_arr = []\n",
    "    for category in arr:\n",
    "        st_cat = stemmer.stem(category)\n",
    "        for word in st_cat.split():\n",
    "            if not word in USELESS_CATEGORIES:\n",
    "                n_arr.append(word)\n",
    "    return ' , '.join(n_arr)\n",
    "df_cluster = df.copy()\n",
    "df_cluster['allCategories'] = df_cluster['categories'].apply(removeUselessCategories)\n",
    "df_cluster['allCategories'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " cooki\n",
      " dessert\n",
      " fruit\n",
      " international\n",
      " drop\n",
      " bar\n",
      " christmas\n",
      " italian\n",
      " chocolate\n",
      " nut\n",
      "\n",
      "Cluster 1:\n",
      " appet\n",
      " spread\n",
      " dips\n",
      " cheese\n",
      " snack\n",
      " appetizers\n",
      " salsa\n",
      " dip\n",
      " mexican\n",
      " bean\n",
      "\n",
      "Cluster 2:\n",
      " pasta\n",
      " shap\n",
      " italian\n",
      " minute\n",
      " chees\n",
      " lasagna\n",
      " everyday\n",
      " macaroni\n",
      " vegetarian\n",
      " 30\n",
      "\n",
      "Cluster 3:\n",
      " soup\n",
      " soups\n",
      " chili\n",
      " stew\n",
      " stews\n",
      " canned\n",
      " chicken\n",
      " pea\n",
      " everyday\n",
      " beef\n",
      "\n",
      "Cluster 4:\n",
      " chicken\n",
      " breast\n",
      " baked\n",
      " roasted\n",
      " everyday\n",
      " low\n",
      " asian\n",
      " style\n",
      " minute\n",
      " pasta\n",
      "\n",
      "Cluster 5:\n",
      " seafood\n",
      " everyday\n",
      " vegetable\n",
      " low\n",
      " potato\n",
      " fish\n",
      " canned\n",
      " salmon\n",
      " squash\n",
      " mexican\n",
      "\n",
      "Cluster 6:\n",
      " vegetarian\n",
      " indian\n",
      " vegan\n",
      " pea\n",
      " bean\n",
      " grain\n",
      " everyday\n",
      " calorie\n",
      " asian\n",
      " tofu\n",
      "\n",
      "Cluster 7:\n",
      " cooki\n",
      " dessert\n",
      " chocolate\n",
      " christmas\n",
      " extract\n",
      " vanilla\n",
      " browni\n",
      " drop\n",
      " bar\n",
      " extracts\n",
      "\n",
      "Cluster 8:\n",
      " breakfast\n",
      " brunch\n",
      " egg\n",
      " potato\n",
      " seafood\n",
      " pancak\n",
      " meat\n",
      " casserol\n",
      " bread\n",
      " everyday\n",
      "\n",
      "Cluster 9:\n",
      " cak\n",
      " dessert\n",
      " cake\n",
      " mix\n",
      " chocolate\n",
      " holiday\n",
      " fruit\n",
      " pumpkin\n",
      " cupcak\n",
      " everyday\n",
      "\n",
      "Cluster 10:\n",
      " dress\n",
      " stuffing\n",
      " thanksgiving\n",
      " sausage\n",
      " fall\n",
      " bread\n",
      " everyday\n",
      " oyster\n",
      " apple\n",
      " cornbread\n",
      "\n",
      "Cluster 11:\n",
      " drink\n",
      " smoothi\n",
      " cocktail\n",
      " breakfast\n",
      " ingredient\n",
      " punch\n",
      " banana\n",
      " fruit\n",
      " everyday\n",
      " strawberry\n",
      "\n",
      "Cluster 12:\n",
      " dessert\n",
      " pi\n",
      " fruit\n",
      " pie\n",
      " candi\n",
      " chocolate\n",
      " everyday\n",
      " apple\n",
      " specialty\n",
      " cream\n",
      "\n",
      "Cluster 13:\n",
      " pork\n",
      " chop\n",
      " calorie\n",
      " rib\n",
      " steaks\n",
      " low\n",
      " 300\n",
      " cooker\n",
      " slow\n",
      " roast\n",
      "\n",
      "Cluster 14:\n",
      " salad\n",
      " pasta\n",
      " chicken\n",
      " vegetable\n",
      " everyday\n",
      " summer\n",
      " potato\n",
      " green\n",
      " lettuce\n",
      " veget\n",
      "\n",
      "Cluster 15:\n",
      " appet\n",
      " snack\n",
      " appetizers\n",
      " poultry\n",
      " meat\n",
      " cheese\n",
      " everyday\n",
      " big\n",
      " game\n",
      " easy\n",
      "\n",
      "Cluster 16:\n",
      " bread\n",
      " muffin\n",
      " quick\n",
      " breakfast\n",
      " yeast\n",
      " holiday\n",
      " brunch\n",
      " pumpkin\n",
      " everyday\n",
      " fruit\n",
      "\n",
      "Cluster 17:\n",
      " casserol\n",
      " beef\n",
      " noodle\n",
      " ground\n",
      " pasta\n",
      " chicken\n",
      " vegetable\n",
      " mexican\n",
      " everyday\n",
      " canned\n",
      "\n",
      "Cluster 18:\n",
      " beef\n",
      " steak\n",
      " ground\n",
      " carb\n",
      " low\n",
      " mexican\n",
      " everyday\n",
      " meatloaf\n",
      " bbq\n",
      " calorie\n",
      "\n",
      "Cluster 19:\n",
      " turkey\n",
      " thanksgiving\n",
      " leftov\n",
      " ground\n",
      " everyday\n",
      " sandwich\n",
      " burg\n",
      " burger\n",
      " july\n",
      " 4th\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def makePipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words='english')),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', KMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def trainModel(X):\n",
    "    model = makePipeline()\n",
    "    model.fit(X)\n",
    "    return model\n",
    "\n",
    "model = trainModel(df_cluster['allCategories'].values)\n",
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "vectorizer = model.named_steps['vect']\n",
    "km = model.named_steps['clf']\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "true_k = 20\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print \"Cluster %d:\" % i\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print ' %s' % terms[ind]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the cluster index to the model\n",
    "df['category_cluster_index'] = model.predict(df['allCategories'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3960      2\n",
       "14340     9\n",
       "2202      2\n",
       "3933      2\n",
       "2256     11\n",
       "Name: category_cluster_index, dtype: int32"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_cluster_index'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model on the 20 cluster indexes\n",
    "Here we are using the cluster indices as labels and concating the ingredient list together to create the features. We then train a RNN classification model. The first layer of our sequential model use the embedding of size 100, the second layer is a LSTM layer, we also added a dropout layer of 0.4. The final layer is a softmax layer to map it to one of our 20 indices. \n",
    "\n",
    "We run the model for 10 epochs and achieved an accuracy of 0.78. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] num of labels: 20\n",
      "[ 1  1  5  0  4  3 16  6  4 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuqinghe/anaconda/envs/py27/lib/python2.7/site-packages/keras/preprocessing/text.py:139: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5486 unique tokens.\n",
      "('Shape of data tensor:', (89061, 100))\n",
      "('Shape of label tensor:', (89061,))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 1,082,420\n",
      "Trainable params: 1,082,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "71249/71249 [==============================] - 474s - loss: 2.1189 - acc: 0.3357   \n",
      "Epoch 2/10\n",
      "71249/71249 [==============================] - 419s - loss: 1.5078 - acc: 0.5331   \n",
      "Epoch 3/10\n",
      "71249/71249 [==============================] - 417s - loss: 1.2719 - acc: 0.6101   \n",
      "Epoch 4/10\n",
      "71249/71249 [==============================] - 414s - loss: 1.1060 - acc: 0.6605   \n",
      "Epoch 5/10\n",
      "71249/71249 [==============================] - 527s - loss: 1.0018 - acc: 0.6907   \n",
      "Epoch 6/10\n",
      "71249/71249 [==============================] - 486s - loss: 0.9218 - acc: 0.7137   \n",
      "Epoch 7/10\n",
      "71249/71249 [==============================] - 467s - loss: 0.8562 - acc: 0.7363   \n",
      "Epoch 8/10\n",
      "71249/71249 [==============================] - 433s - loss: 0.8028 - acc: 0.7504   \n",
      "Epoch 9/10\n",
      "71249/71249 [==============================] - 427s - loss: 0.7532 - acc: 0.7645   \n",
      "Epoch 10/10\n",
      "71249/71249 [==============================] - 871s - loss: 0.7029 - acc: 0.7799   \n",
      "17812/17812 [==============================] - 115s   \n",
      "[0.81701629787730334, 0.74814731644266996]\n"
     ]
    }
   ],
   "source": [
    "def concatIngredients(arr):\n",
    "    return ','.join(arr).encode('ascii', 'ignore')\n",
    "\n",
    "def processKerasModel(df_):\n",
    "    embedding_length = 100\n",
    "    top_words = 10000\n",
    "    df = df_.copy()\n",
    "    df['features'] = df['ingredients'].apply(concatIngredients)\n",
    "    df['label'] = df['category_cluster_index']\n",
    "    df = df[df['label'].astype(str) != 'nan']\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    features = df['features'].values\n",
    "    labels = df['label'].values\n",
    "    \n",
    "    le.fit(np.unique(labels))\n",
    "    print list(le.classes_), 'num of labels:',len(np.unique(labels))\n",
    "    labels = le.transform(labels) \n",
    "    print labels[:10]\n",
    "#     X_train, Y_train, X_test, Y_test = ut.simpleSplit(features, labels)\n",
    "\n",
    "    tokenizer = Tokenizer(nb_words=top_words)\n",
    "    tokenizer.fit_on_texts(features)\n",
    "    sequences = tokenizer.texts_to_sequences(features)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=embedding_length)\n",
    "\n",
    "#     labels = np_utils.to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    # split the data into a training set and a validation set\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "    X_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    X_test = data[-nb_validation_samples:]\n",
    "    y_test = labels[-nb_validation_samples:]\n",
    "    \n",
    "    embedding_vecor_length = 100\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vecor_length, input_length=embedding_length))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(LSTM(100)) #, return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dense(len(np.unique(labels)), activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "    # Final evaluation of the model\n",
    "    return model, X_test, y_test\n",
    "\n",
    "rnn_cluster_model, X_test, y_test = processKerasModel(df_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy on the train set is 0.78, the accuracy on the test set is 0.75. In total the model took about 2 hours to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for future use\n",
    "saveKerasModel('model_top_20_clustering_index.json',rnn_cluster_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model if needed\n",
    "# t_model = loadKerasModel('model_top_20_clustering_index.json')\n",
    "# t_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification  Model 2: Top 30 Categories by Index\n",
    "In this model, we have first extract the top 30 categories by doing counting the occurency of each category. Then we set the label of each recipe to one of the top 30 categories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "unique_cat = Counter()\n",
    "USELESS_CATEGORIES = ['recipes']\n",
    "for idx, row in df.iterrows():\n",
    "    for cat in row['categories']:\n",
    "        cat = cat.lower()\n",
    "        if not cat in USELESS_CATEGORIES:\n",
    "            unique_cat[cat]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'everyday cooking',\n",
       " u'main dishes',\n",
       " u'desserts',\n",
       " u'appetizers and snacks',\n",
       " u'pasta by shape',\n",
       " u'chicken breasts',\n",
       " u'asian recipes',\n",
       " u'side dishes',\n",
       " u'pasta main dishes',\n",
       " u'soups, stews and chili',\n",
       " u'fruit desserts',\n",
       " u'italian recipes',\n",
       " u'salad recipes',\n",
       " u'bread recipes',\n",
       " u'breakfast and brunch',\n",
       " u'mexican recipes',\n",
       " u'cake recipes',\n",
       " u'faceless recipes',\n",
       " u'drinks',\n",
       " u'cookies',\n",
       " u'soup',\n",
       " u'trusted brands',\n",
       " u'baked and roasted chicken',\n",
       " u'dips and spreads',\n",
       " u'vegan recipes',\n",
       " u'fish recipes',\n",
       " u'casseroles',\n",
       " u'canned beans and peas',\n",
       " u'vanilla extract',\n",
       " u'cheese appetizers']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_K_CAT = 30\n",
    "k_cat = [cat[0] for cat in unique_cat.most_common(TOP_K_CAT)]\n",
    "k_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map each recipe to a category\n",
    "since each recipe contains multiple categories, we loop through the categories list for each recipe, and map the recipe to ONE category in the TOP_K_CAT list. If a recipe doesn't belong in any category, we map it to 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "everyday cooking             12170\n",
       "side dishes                   5847\n",
       "fruit desserts                4884\n",
       "salad recipes                 4733\n",
       "asian recipes                 4434\n",
       "chicken breasts               4350\n",
       "breakfast and brunch          3999\n",
       "italian recipes               3761\n",
       "appetizers and snacks         3679\n",
       "desserts                      3633\n",
       "trusted brands                3416\n",
       "soup                          3366\n",
       "bread recipes                 3270\n",
       "cookies                       3180\n",
       "dips and spreads              3033\n",
       "pasta by shape                3030\n",
       "drinks                        2903\n",
       "cake recipes                  2490\n",
       "main dishes                   2264\n",
       "mexican recipes               2220\n",
       "soups, stews and chili        2099\n",
       "fish recipes                  1812\n",
       "cheese appetizers             1318\n",
       "casseroles                    1201\n",
       "baked and roasted chicken      675\n",
       "pasta main dishes              497\n",
       "vegan recipes                  440\n",
       "other                          348\n",
       "vanilla extract                  9\n",
       "Name: cat_label, dtype: int64"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapToSingleCategory(arr):\n",
    "    for item in arr:\n",
    "        if item.lower() in k_cat:\n",
    "            return item.lower()\n",
    "    return 'other'\n",
    "df_single_cat = df.copy()\n",
    "df_single_cat['cat_label'] = df_single_cat['categories'].apply(mapToSingleCategory)\n",
    "df_single_cat['cat_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embedding using the unique_ingredients\n",
    "We have already created a list of unique_ingredients, so we just need to read it in and map each recipe to a one-hot vector. This takes a long time so we processed and stored the word embedding matrix in sparse_recipe_ingredient_matrix.npz. We just need to map those to the categories for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe_ingredient_matrix shape: (89061, 19013)\n",
      "<type 'numpy.ndarray'>\n",
      "reduced_recipe_ingredient_matrix shape: (89061, 50)\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 cp s3://RecipeVectors/sparse_recipe_ingredient_matrix.npz .\n",
    "# https://stackoverflow.com/a/8980156/2491761\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])\n",
    "\n",
    "recipe_ingredient_matrix = load_sparse_csr(\"sparse_recipe_ingredient_matrix.npz\")\n",
    "print 'recipe_ingredient_matrix shape:', recipe_ingredient_matrix.shape\n",
    "\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "reduced_recipe_ingredient_matrix = svd.fit_transform(recipe_ingredient_matrix)\n",
    "\n",
    "print type(reduced_recipe_ingredient_matrix)\n",
    "print 'reduced_recipe_ingredient_matrix shape:', reduced_recipe_ingredient_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>cat_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19139</th>\n",
       "      <td>[1.22050713893, 1.76628974185, 0.0354851312098...</td>\n",
       "      <td>bread recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24589</th>\n",
       "      <td>[1.89889819297, -0.686912104969, -0.4275120361...</td>\n",
       "      <td>breakfast and brunch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>[1.48215031547, 1.68507335044, -0.462875380731...</td>\n",
       "      <td>fruit desserts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25220</th>\n",
       "      <td>[1.12445043427, 0.796921282012, -0.22731742225...</td>\n",
       "      <td>bread recipes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>[2.50889096518, -1.6057192107, -1.01385053416,...</td>\n",
       "      <td>main dishes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                features             cat_label\n",
       "19139  [1.22050713893, 1.76628974185, 0.0354851312098...         bread recipes\n",
       "24589  [1.89889819297, -0.686912104969, -0.4275120361...  breakfast and brunch\n",
       "1085   [1.48215031547, 1.68507335044, -0.462875380731...        fruit desserts\n",
       "25220  [1.12445043427, 0.796921282012, -0.22731742225...         bread recipes\n",
       "2125   [2.50889096518, -1.6057192107, -1.01385053416,...           main dishes"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single_cat['features'] = list(reduced_recipe_ingredient_matrix)\n",
    "df_single_cat[['features','cat_label']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode the labels\n",
    "Here we use sklearn's preprocessing to encode the 30 labels into indices; the final softmax layer in our classification model will be a vector of length 30, corresponding to the probability of each label for a given input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'appetizers and snacks' u'asian recipes' u'baked and roasted chicken'\n",
      " u'bread recipes' u'breakfast and brunch' u'cake recipes' u'casseroles'\n",
      " u'cheese appetizers' u'chicken breasts' u'cookies' u'desserts'\n",
      " u'dips and spreads' u'drinks' u'everyday cooking' u'fish recipes'\n",
      " u'fruit desserts' u'italian recipes' u'main dishes' u'mexican recipes'\n",
      " 'other' u'pasta by shape' u'pasta main dishes' u'salad recipes'\n",
      " u'side dishes' u'soup' u'soups, stews and chili' u'trusted brands'\n",
      " u'vanilla extract' u'vegan recipes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_single_cat['cat_label'])\n",
    "print le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7506</th>\n",
       "      <td>[0.251008849796, 0.454623460639, 0.12009385247...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4543</th>\n",
       "      <td>[1.03191109934, 2.11129561861, -0.024014297480...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>[2.22011559852, -0.470587151997, 0.23649744678...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867</th>\n",
       "      <td>[4.73585551979, -0.43068586289, -0.31929311014...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10511</th>\n",
       "      <td>[0.398673862123, -0.135033842398, 0.0708437121...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                features  label\n",
       "7506   [0.251008849796, 0.454623460639, 0.12009385247...     15\n",
       "4543   [1.03191109934, 2.11129561861, -0.024014297480...     10\n",
       "10785  [2.22011559852, -0.470587151997, 0.23649744678...      6\n",
       "8867   [4.73585551979, -0.43068586289, -0.31929311014...     13\n",
       "10511  [0.398673862123, -0.135033842398, 0.0708437121...     22"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single_cat['label'] = le.transform(df_single_cat['cat_label'])\n",
    "df_final = df_single_cat[['features','label']]\n",
    "df_final.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'casseroles', u'drinks', u'appetizers and snacks'], dtype=object)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the encoding works\n",
    "le.inverse_transform([6,12,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalized Dataset\n",
    "The final dataset is split into Train/Test sets,  we can now pipe this through a CNN model with softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_train shape: (71248, 50)\n",
      "y_train shape: (71248,)\n",
      "X_test shape: (17813, 50)\n",
      "y_test shape: (17813,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features = np.array([np.array(row) for row in df_final['features'].values])\n",
    "labels = df_final['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20)\n",
    "print 'X_train shape:', X_train.shape\n",
    "print 'y_train shape:', y_train.shape\n",
    "print 'X_test shape:', X_test.shape\n",
    "print 'y_test shape:', y_test.shape\n",
    "# print 'sample X:', np.random.choice(X_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (71248, 50))\n",
      "('Shape of label tensor:', (71248,))\n",
      "Training model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 50, 128)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 46, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1, 128)            16512     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1, 128)            16512     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1, 89061)          11488869  \n",
      "=================================================================\n",
      "Total params: 11,604,197\n",
      "Trainable params: 11,604,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_75_input to have 3 dimensions, but got array with shape (71248, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-323-c72611a231e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mconv_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainKerasModel_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-323-c72611a231e0>\u001b[0m in \u001b[0;36mtrainKerasModel_conv\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 metrics=['acc'])\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chuqinghe/anaconda/envs/py27/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/chuqinghe/anaconda/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chuqinghe/anaconda/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1232\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1235\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1236\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chuqinghe/anaconda/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_75_input to have 3 dimensions, but got array with shape (71248, 50)"
     ]
    }
   ],
   "source": [
    "# global max pooling : https://www.quora.com/What-is-global-average-pooling\n",
    "# CNN for text classification: https://arxiv.org/pdf/1408.5882.pdf\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def trainKerasModel_conv(X,Y):\n",
    "    print('Shape of data tensor:', X.shape)\n",
    "    print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "    print('Training model.')\n",
    "    BATCH_SIZE = 50\n",
    "    # train a 1D convnet with global maxpooling\n",
    "#     inp = Input(shape=(None,MAX_SEQUENCE_LENGTH,))\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(MAX_SEQUENCE_LENGTH), activation='relu'))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(35))  # global max pooling\n",
    "    model.add(Conv1D(128, 1, activation='relu'))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['acc'])\n",
    "    print model.summary()\n",
    "    model.fit(X, Y, epochs=2, batch_size=BATCH_SIZE)\n",
    "    # Final evaluation of the model\n",
    "    return model\n",
    "\n",
    "conv_model = trainKerasModel_conv(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model 3\n",
    "Let's do something fun and generate the recipes from scratch :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'categories', u'cookingTime', u'description', u'ingredients',\n",
       "       u'instructionSteps', u'name', u'rating', u'ratingCount', u'url',\n",
       "       'cookingTimeMinutes', 'cleanedIngredients', 'allCategories',\n",
       "       'category_cluster_index'], dtype=object)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def processCategories(arr):\n",
    "    arr = [re.sub(r'[\\xa0\\x99]', ' ', k.encode('ascii', 'ignore')) for k in arr]\n",
    "    return '<category>' + ','.join(arr) + '</category>'\n",
    "def processIngredients(arr):\n",
    "    arr = [re.sub(r'[\\xa0\\x99]', ' ', k.encode('ascii', 'ignore')) for k in arr]\n",
    "    return '<ingredients>' + ','.join(arr) + '</ingredients>'\n",
    "def processName(text):\n",
    "    return '<name> '+ text+' </name>'\n",
    "def processDesc(text):\n",
    "    if text == text:\n",
    "        text = re.sub(r'[\\xa0\\x99]', ' ', text.encode('ascii', 'ignore'))\n",
    "        return '<desc> '+ str(text) +' </desc>'\n",
    "    return '<no_desc>'\n",
    "def processInstructions(arr):\n",
    "    if not arr == arr:\n",
    "        return '<no_inst>'\n",
    "    retStr = '<inst> '\n",
    "    for i, inst in enumerate(arr):\n",
    "        inst = re.sub(r'[\\xa0\\x99]', ' ', inst.encode('ascii', 'ignore'))\n",
    "        retStr += (str(i) +' : '+inst + ' ')\n",
    "    retStr += ' </inst>'\n",
    "    return retStr\n",
    "def getRating(text):\n",
    "    return '<rating> ' + str(text) + ' </rating>' \n",
    "def addRowsTogether(row):\n",
    "    return ' '.join(row[['ingredients', 'name', 'description','instructionSteps','categories','rating']])\n",
    "\n",
    "\n",
    "df_recipe_generator = df.copy()\n",
    "df_recipe_generator['categories'] = df_recipe_generator['categories'].apply(processCategories)\n",
    "df_recipe_generator['name'] = df_recipe_generator['name'].apply(processName)\n",
    "df_recipe_generator['ingredients'] = df_recipe_generator['ingredients'].apply(processIngredients)\n",
    "df_recipe_generator['description'] = df_recipe_generator['description'].apply(processDesc)\n",
    "df_recipe_generator['rating'] = df_recipe_generator['rating'].apply(getRating)\n",
    "df_recipe_generator['instructionSteps'] = df_recipe_generator['instructionSteps'].apply(processInstructions)\n",
    "\n",
    "df_recipe_generator['total_recipe'] = df_recipe_generator.apply(addRowsTogether, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1935     <ingredients>2 cups fresh broccoli florets,1 t...\n",
       "20691    <ingredients>3 cups 1-inch pieces rhubarb,1 1/...\n",
       "2132     <ingredients>1 tablespoon vegetable oil,4 bone...\n",
       "11233    <ingredients>cooking spray,1 cup almond flour,...\n",
       "27386    <ingredients>1 cup packed brown sugar,1/4 cup ...\n",
       "Name: total_recipe, dtype: object"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recipe_generator['total_recipe'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df_recipe_generator['total_recipe'].values\n",
    "chars = list(set(data))\n",
    "VOCAB_SIZE = len(chars)\n",
    "SEQ_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(data)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE))\n",
    "y = np.zeros((len(data)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE))\n",
    "for i in range(0, len(data)/SEQ_LENGTH):\n",
    "    X_sequence = data[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = data[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n",
    "    y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatIngredients(arr):\n",
    "    return ','.join(arr).encode('ascii', 'ignore')\n",
    "\n",
    "def processKerasModel3(df_):\n",
    "    embedding_length = 100\n",
    "    top_words = 10000\n",
    "    df = df_.copy()\n",
    "    df['features'] = df['ingredients'].apply(concatIngredients)\n",
    "    df['label'] = df['category_cluster_index']\n",
    "    df = df[df['label'].astype(str) != 'nan']\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    features = df['features'].values\n",
    "    labels = df['label'].values\n",
    "    \n",
    "    le.fit(np.unique(labels))\n",
    "    print list(le.classes_), 'num of labels:',len(np.unique(labels))\n",
    "    labels = le.transform(labels) \n",
    "    print labels[:10]\n",
    "#     X_train, Y_train, X_test, Y_test = ut.simpleSplit(features, labels)\n",
    "\n",
    "    tokenizer = Tokenizer(nb_words=top_words)\n",
    "    tokenizer.fit_on_texts(features)\n",
    "    sequences = tokenizer.texts_to_sequences(features)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=embedding_length)\n",
    "\n",
    "#     labels = np_utils.to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    # split the data into a training set and a validation set\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "    X_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    X_test = data[-nb_validation_samples:]\n",
    "    y_test = labels[-nb_validation_samples:]\n",
    "    \n",
    "    embedding_vecor_length = 100\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vecor_length, input_length=embedding_length))\n",
    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dense(len(np.unique(labels)), activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "    # Final evaluation of the model\n",
    "    return model, X_test, y_test\n",
    "\n",
    "rnn_generative_model, X_test, y_test = processKerasModel3(df_recipe_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, length):\n",
    "    ix = [np.random.randint(VOCAB_SIZE)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, VOCAB_SIZE))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
