{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!aws s3 cp s3://RecipeVectors/sparse_recipe_ingredient_matrix.npz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/8980156/2491761\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])\n",
    "\n",
    "recipe_ingredient_matrix = load_sparse_csr(\"sparse_recipe_ingredient_matrix.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "6669250\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=50)\n",
    "reduced_recipe_ingredient_matrix = svd.fit_transform(recipe_ingredient_matrix)\n",
    "\n",
    "print type(reduced_recipe_ingredient_matrix)\n",
    "print reduced_recipe_ingredient_matrix.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!aws s3 cp s3://RecipeVectors/unique_ingredients.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('unique_ingredients.pkl', 'rb') as f:\n",
    "    unique_ingredients = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!aws s3 cp s3://RecipeVectors/CleanedIngredients.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('CleanedIngredients.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makePipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', KMeans(n_clusters=2, init='k-means++', max_iter=100, n_init=1))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def trainModel(X, Y):\n",
    "    model = makePipeline()\n",
    "    model.fit(X, Y)\n",
    "    return model\n",
    "\n",
    "X = [' '.join(f) for f in df['ingredients'].values]\n",
    "Y = df['categories'].values   # Roy: I think this needs to change to a \n",
    "\n",
    "model = trainModel(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_k = len(np.unique(Y))\n",
    "true_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq = np.unique(Y)\n",
    "pred_labeled = [uniq[1] if p == 0 else uniq[0] for p in preds]\n",
    "print confusion_matrix(pred_labeled, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print classification_report(pred_labeled, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "vectorizer = model.named_steps['vect']\n",
    "km = model.named_steps['clf']\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print \"Cluster %d:\" % i\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print ' %s' % terms[ind]\n",
    "    print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMultiClassData():\n",
    "    filename2 = './recipeVectors/allRecipes_recipes.json'\n",
    "#     filename2 = '../../data/cleandata/sunbasket_noapp.csv'\n",
    "    df = pd.read_json(filename2)\n",
    "    return df\n",
    "\n",
    "def concatIngredients(arr):\n",
    "    return ','.join(arr).encode('ascii', 'ignore')\n",
    "\n",
    "\n",
    "def getTopCategory(arr):\n",
    "    if arr == arr and len(arr) > 0:\n",
    "        return arr[0].encode('ascii', 'ignore')\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def processKerasModel(df_):\n",
    "    embedding_length = 100\n",
    "    top_words = 10000\n",
    "    df = df_.copy()\n",
    "    df['features'] = df['ingredients'].apply(concatIngredients)\n",
    "    df['label'] = df['categories'].apply(getTopCategory)\n",
    "    df = df[df['label'].astype(str) != 'nan']\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    features = df['features'].values\n",
    "    labels = df['label'].values\n",
    "    \n",
    "    le.fit(np.unique(labels))\n",
    "    print list(le.classes_), 'num of labels:',len(np.unique(labels))\n",
    "    labels = le.transform(labels) \n",
    "    print labels[:10]\n",
    "#     X_train, Y_train, X_test, Y_test = ut.simpleSplit(features, labels)\n",
    "\n",
    "    tokenizer = Tokenizer(nb_words=top_words)\n",
    "    tokenizer.fit_on_texts(features)\n",
    "    sequences = tokenizer.texts_to_sequences(features)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=embedding_length)\n",
    "\n",
    "#     labels = np_utils.to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    # split the data into a training set and a validation set\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "    X_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    X_test = data[-nb_validation_samples:]\n",
    "    y_test = labels[-nb_validation_samples:]\n",
    "    \n",
    "    embedding_vecor_length = 100\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vecor_length, input_length=embedding_length))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(LSTM(100)) #, return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dense(len(np.unique(labels)), activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "#         loss='sparse_categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "                optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "#               metrics=['accuracy'])\n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "    # Final evaluation of the model\n",
    "    return model, X_test, y_test\n",
    "finalset = getMultiClassData()\n",
    "\n",
    "print finalset.sample(5)\n",
    "\n",
    "dl_model, X_test, y_test = processKerasModel(finalset)\n",
    "scores = dl_model.evaluate(X_test, y_test, verbose=1)  \n",
    "print scores\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
