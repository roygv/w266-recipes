{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecipeToVec\n",
    "Word to vec using gensim: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "Here we are creating the word vectors from the recipes.\n",
    "Each document is one recipe's list of clean ingredients + verbs. \n",
    "We use the Gensim model to create the similarity matrix(cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "#from gensim.models.word2vec import Word2Vec\n",
    "from gensim import corpora, models, utils\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enable logging (for gensim)\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "cleanerIngredientsDict=pd.read_pickle('cleanerIngredients.pkl')  \n",
    "verbsDict=pd.read_pickle('verbs.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48417\n",
      "48417\n",
      "[u'heat', u'stir', u'cook', u'mix', u'stir', u'begin', u'reduce', u'cover', u'simmer', u'absorb']\n"
     ]
    }
   ],
   "source": [
    "print len(cleanerIngredientsDict)\n",
    "print len(verbsDict)\n",
    "print verbsDict['http://allrecipes.com/recipe/33385/best-spanish-rice/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allRecipes=pd.read_pickle('CleanedIngredients.pkl')\n",
    "allRecipes.drop_duplicates(subset='url', keep='first', inplace=True)\n",
    "indexedRecipes=allRecipes.set_index(\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipes=[]\n",
    "names=[]\n",
    "categories=[]\n",
    "for idx, row in allRecipes.iterrows():\n",
    "    name = [word for word in row[\"name\"].lower().split()]\n",
    "    url = row[\"url\"]\n",
    "    verbs = verbsDict[url]\n",
    "    if type(verbs) != list:\n",
    "        print url\n",
    "        print '%s is not a list' % verbs\n",
    "        verbs=[]\n",
    "    ingredients = list(cleanerIngredientsDict[url])\n",
    "    # Concatenate phrases into single tokens\n",
    "    # ingredients=[i.replace(' ','_') for i in ingredients]\n",
    "    recipes.append(name + ingredients + verbs)\n",
    "    names.append(row[\"name\"])\n",
    "    categories.append(row[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>cookingTime</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructionSteps</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>ratingCount</th>\n",
       "      <th>url</th>\n",
       "      <th>cookingTimeMinutes</th>\n",
       "      <th>cleanedIngredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16507</th>\n",
       "      <td>[Whiskey Drinks, Rum Drinks, Cocktails, Drinks...</td>\n",
       "      <td>PT5M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ice, 1 fluid ounce coconut flavored rum, 1/2 ...</td>\n",
       "      <td>[Fill a cocktail shaker with ice. Pour in the ...</td>\n",
       "      <td>Wendy's Drunken Snow Cone</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>http://allrecipes.com/recipe/154624/wendys-dru...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[ice, coconut flavored rum, chambord raspberry...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              categories cookingTime  \\\n",
       "16507  [Whiskey Drinks, Rum Drinks, Cocktails, Drinks...        PT5M   \n",
       "\n",
       "      description                                        ingredients  \\\n",
       "16507         NaN  [ice, 1 fluid ounce coconut flavored rum, 1/2 ...   \n",
       "\n",
       "                                        instructionSteps  \\\n",
       "16507  [Fill a cocktail shaker with ice. Pour in the ...   \n",
       "\n",
       "                            name  rating  ratingCount  \\\n",
       "16507  Wendy's Drunken Snow Cone     5.0            2   \n",
       "\n",
       "                                                     url  cookingTimeMinutes  \\\n",
       "16507  http://allrecipes.com/recipe/154624/wendys-dru...                 5.0   \n",
       "\n",
       "                                      cleanedIngredients  \n",
       "16507  [ice, coconut flavored rum, chambord raspberry...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allRecipes.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recipes loaded: 48417 \n",
      "[u'fresco', u'salsa', u'tomato', u'lime juice', u'cilantro', u'red bell pepper', u'onion', u'yellow bell pepper', u'salt', u'mix', u'lime', u'cover', u'refrigerate', u'serve']\n"
     ]
    }
   ],
   "source": [
    "print 'Total recipes loaded: %s ' % len(recipes)\n",
    "print recipes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(24739 unique tokens: [u'', u'butter flavoring', u'gai', u'blast-off', u'gag']...)\n",
      "The token ID of milk is: 62 \n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary and save it\n",
    "dictionary = corpora.Dictionary(recipes)\n",
    "dictionary.save('recipe2vec.dict')\n",
    "print(dictionary)\n",
    "print \"The token ID of milk is: %s \" % dictionary.token2id[\"milk\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a corpus and save it\n",
    "corpus = [dictionary.doc2bow(recipe) for recipe in recipes]\n",
    "corpora.MmCorpus.serialize('recipe2vec.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "if (os.path.exists(\"recipe2vec.dict\")):\n",
    "    dictionary = corpora.Dictionary.load('recipe2vec.dict')\n",
    "    corpus = corpora.MmCorpus('recipe2vec.mm')\n",
    "    print(\"Loaded dictionary and corpus from disk\")\n",
    "else:\n",
    "    print(\"Error: Could find dictionary \\\"recipe2vec.dict\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tfidf = models.TfidfModel(corpus, normalize=True)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48417\n"
     ]
    }
   ],
   "source": [
    "print len(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument([u'fresco', u'salsa', u'tomato', u'lime juice', u'cilantro', u'red bell pepper', u'onion', u'yellow bell pepper', u'salt', u'mix', u'lime', u'cover', u'refrigerate', u'serve'], [0])\n"
     ]
    }
   ],
   "source": [
    "taggedRecipes=[models.doc2vec.TaggedDocument(recipes[i], [i]) for i in range(len(recipes))]\n",
    "print taggedRecipes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Doc2vec model...\n"
     ]
    }
   ],
   "source": [
    "print('Training a Doc2vec model...')\n",
    "w2v_model = models.doc2vec.Doc2Vec(taggedRecipes, size=100, window=4, min_count=5, workers=4, iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Let us see what this looks like...')\n",
    "print taggedRecipes[0]\n",
    "print  w2v_model.infer_vector(taggedRecipes[0].words)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"We can turn enything into a vector now\"\n",
    "print  w2v_model.infer_vector([u'chicken',u'masala'])[:10]\n",
    "\n",
    "print \"This is what the documents look like:\"\n",
    "for doc_id in range(50,55):\n",
    "    print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(taggedRecipes[doc_id].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model.docvecs.most_similar([w2v_model.infer_vector(taggedRecipes[0].words)], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well of a representation is it? Are documents the most similar to themselves?  \n",
    "Let see how many documents the model thinks are the most similar to themselves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm as ProgressBar\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in ProgressBar(range(len(taggedRecipes)), desc=\"Processing recipes\"):\n",
    "    inferred_vector = w2v_model.infer_vector(taggedRecipes[doc_id].words)\n",
    "    sims = w2v_model.docvecs.most_similar([inferred_vector], topn=len(w2v_model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])\n",
    "    if doc_id==10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "c= collections.Counter(ranks)\n",
    "print \"{}% of the recipes were the model's top match. For another {}% they were #2.\".format(c[0]/10000.0*100, c[1]/10000.0*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results:\n",
    "0: 83.85%, 1:4.53%  size=100, window=7, min_count=2, workers=4, iter=50  \n",
    "0: 68.84%  1:5.31%  size=100, window=7, min_count=5, workers=4, iter=30  \n",
    "size=100, window=4, min_count=5, workers=4, iter=30)\n",
    "size=100, window=4, min_count=30, workers=4, iter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search by URL (find recipes like this one)  \n",
    "Here we look for for the parsed recipe in our data frame. In a more realistic (and useful) implementation, the recipe will be read and parsed and a BOW (ingredients, actions, name) will be used to look for similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_id=indexedRecipes.index.get_loc('http://allrecipes.com/recipe/166638/baked-buffalo-wings/')\n",
    "bag_of_words=taggedRecipes[doc_id].words\n",
    "inferred_vector = w2v_model.infer_vector(bag_of_words)\n",
    "sims=w2v_model.docvecs.most_similar([inferred_vector], topn=len(w2v_model.docvecs))\n",
    "print allRecipes.iloc[doc_id][\"name\"]\n",
    "print allRecipes.iloc[doc_id][\"url\"]\n",
    "print 'Best match: '\n",
    "print allRecipes.iloc[sims[0][0]][\"name\"]\n",
    "print allRecipes.iloc[sims[0][0]][\"url\"]\n",
    "print 'Next best: '\n",
    "print allRecipes.iloc[sims[1][0]][\"name\"]\n",
    "print allRecipes.iloc[sims[1][0]][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsVec=utils.simple_preprocess(u'chickem tikka masala')\n",
    "inferred_vector = w2v_model.infer_vector(wordsVec)\n",
    "sims=w2v_model.docvecs.most_similar([inferred_vector], topn=len(w2v_model.docvecs))\n",
    "print 'Best match: '\n",
    "print allRecipes.iloc[sims[0][0]][\"name\"]\n",
    "print allRecipes.iloc[sims[0][0]][\"url\"]\n",
    "print 'Next best: '\n",
    "print allRecipes.iloc[sims[1][0]][\"name\"]\n",
    "print allRecipes.iloc[sims[1][0]][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This runs a shell command from the notebook.\n",
    "!pip install plotly\n",
    "\n",
    "# Plotly imports.\n",
    "import plotly.offline as plotly\n",
    "plotly.offline.init_notebook_mode()\n",
    "import plotly.graph_objs as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "data = [go.Scatter(x=v[0][:n], y=v[1][:n], text=names,\n",
    "                   mode='markers', textposition='bottom', hoverinfo='text')]\n",
    "fig = go.Figure(data=data, layout=go.Layout(title=\"Word Embeddings\", hovermode='closest'))\n",
    "plotly.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
